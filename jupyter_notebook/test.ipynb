{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-07T07:11:53.208998600Z",
     "start_time": "2025-05-07T07:11:50.258126200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/openrlhf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/sujin/PycharmProjects/OpenRLHF\")\n",
    "\n",
    "from openrlhf.utils import blending_datasets, get_strategy, get_tokenizer\n",
    "from datasets import load_dataset, concatenate_datasets, interleave_datasets\n",
    "from openrlhf.datasets import SFTDataset, PromptDataset\n",
    "from easydict import EasyDict"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pretrain = \"/sujin/Models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = get_tokenizer(pretrain, None, \"right\", None, use_fast=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T07:12:01.228734200Z",
     "start_time": "2025-05-07T07:12:00.859987900Z"
    }
   },
   "id": "222db6273af7d2fc",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:01<00:00, 571.00 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 77417.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = \"Open-Orca/OpenOrca\"\n",
    "\n",
    "strategy = {\n",
    "    \"args\": {\n",
    "        \"input_key\": \"question\",\n",
    "        \"output_key\": \"response\",\n",
    "        \"input_template\": \"$'User: {}\\nAssistant: '\",\n",
    "        \"apply_chat_template\": False,\n",
    "        \"tokenizer_chat_template\": None\n",
    "    }\n",
    "}\n",
    "strategy = EasyDict(strategy)\n",
    "\n",
    "train_data = load_dataset(dataset)\n",
    "train_data = concatenate_datasets([train_data[\"train\"]])\n",
    "train_data = train_data.select(range(1000))\n",
    "\n",
    "train_dataset = SFTDataset(\n",
    "    train_data,\n",
    "    tokenizer,\n",
    "    4096,\n",
    "    strategy,\n",
    "    pretrain_mode=False,\n",
    "    input_template=strategy[\"args\"][\"input_template\"],\n",
    "    multiturn=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-06T06:38:21.970253700Z",
     "start_time": "2025-05-06T06:38:17.274518500Z"
    }
   },
   "id": "902f565f62af0a62",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[     3,      6,   1474,     25,  29901,    279,  25975,   1447,   1654,\n           19334,    279,    348,   5082,    323,   1730,    432,    311,    387,\n           26291,     13,   2806,  14063,   4113,    911,    419,    830,   3364,\n              11,    358,   3381,     25,    330,  11908,     11,    902,     11,\n             393,   1785,   3630,  18759,    438,    458,   3693,   7748,   4319,\n            1151,      6,   7748,    304,    279,   4531,    701,   1128,    264,\n            3873,   5754,      1,   3080,    358,  11105,    279,   8046,    911,\n           25685,  73695,     13,    576,   4531,   1558,    264,   1661,   2618,\n             315,  44196,    279,  37829,    315,   1493,   9867,  31438,    323,\n           77369,    279,  24207,  50186,    911,   1105,     13,   1597,  51989,\n           10620,   3444,   2863,     11,  61300,     11,   1128,    264,  13143,\n              11,    323,   1602,  39561,    438,    458,   7748,   5220,    320,\n              40,   4411,   1340,    374,   8585,     12,  59574,     26,   1340,\n            2704,    312,  28163,    311,    279,    678,  93579,  11285,   8806,\n             315,   1741,    568,    758,  33681,     11,    315,  33221,  18759,\n             594,  43917,     11,   1081,  26324,   1707,     11,    304,    279,\n             835,    566,   4041,   1526,  35861,  11307,    438,    264,  24425,\n              11,  12235,    883,     13,    576,   7089,    374,    264,   2632,\n            7469,    304,  16661,    495,   1095,    806,  14409,    504,    489,\n            3106,    311,   9864,   7960,   8768,    380,     13,   7684,   4531,\n              11,   7548,  11102,    624,  71703,     25,    364,   1986,    374,\n             264,   6785,  25975,     13,    576,   1697,   2578,    614,   1030,\n            1045,  38917,    518,    279,   7167,     11,   5310,    911,    393,\n              13,  33221,  18759,   5619,    458,   3693,   7748,     11,    714,\n             304,    279,    835,     11,    807,   1730,    279,   4531,  26291,\n             323,   1602,   1661,     13,   2379,  14006,    279,  23675,    315,\n             279,  19571,     11,   5310,  51989,  10620,   3444,   2863,     11,\n             323,  25808,    279,   5700,    594,  73933,    315,   9867,  31438,\n              13,   7418,   3498,    807,   3381,    279,   7089,   1410,    614,\n            2814,    264,   2664,   2618,   9027,    279,   3668,    594,   2297,\n             504,    264,    489,   3106,    311,    458,   9864,  28611,    380,\n              11,    807,   2058,   7548,   6934,    279,   4531,     13,   1084,\n             594,   1075,    979,    498,   1430,    264,    501,  17172,    315,\n            9853,  12644,     11,   1496,    421,    498,  14716,    944,   2704,\n             911,    432,    518,   1156,     11,    498,    835,    705,   2167,\n           48737,    432,    323,   1366,    697,   4780,    311,   1430,    432,\n            2238,      0,    220, 151645]]),\n tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 0.]]))"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-06T08:33:47.703896Z",
     "start_time": "2025-05-06T08:33:47.325391300Z"
    }
   },
   "id": "1eb7b3b49cd668fa",
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e1d28bd3664fcfe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = \"OpenRLHF/prompt-collection-v0.1\"\n",
    "\n",
    "strategy = {\n",
    "    \"args\": {\n",
    "        \"input_key\": \"context_messages\",\n",
    "        \"input_template\": None,\n",
    "        \"apply_chat_template\": True,\n",
    "    },\n",
    "    \"is_rank_0\": lambda: False,\n",
    "}\n",
    "strategy = EasyDict(strategy)\n",
    "\n",
    "train_data = load_dataset(dataset)\n",
    "train_data = concatenate_datasets([train_data[\"train\"]])\n",
    "train_data = train_data.select(range(1000))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T07:16:10.406026400Z",
     "start_time": "2025-05-07T07:16:07.131638700Z"
    }
   },
   "id": "ce5286b60cf44421",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('default', '<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat are the three most important things to consider when deciding what technology to use to build an assist device to help an elderly person with basic needs?<|im_end|>\\n<|im_start|>assistant\\n', '')\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset\n",
    "prompts_dataset = PromptDataset(train_data, tokenizer, strategy, input_template=strategy.args.input_template)\n",
    "print(prompts_dataset.__getitem__(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T07:53:22.502711300Z",
     "start_time": "2025-05-07T07:53:22.080115800Z"
    }
   },
   "id": "afa28fff3df733ff",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-07 10:04:14 model.py:73] set value_head_prefix to `score`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n",
      "Some weights of RewardModel were not initialized from the model checkpoint at /sujin/Models/Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "RewardModel(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2Attention(\n          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (score): Linear(in_features=896, out_features=1, bias=False)\n)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openrlhf.models.model import get_llm_for_sequence_regression\n",
    "\n",
    "\n",
    "get_llm_for_sequence_regression(\"/sujin/Models/Qwen/Qwen2.5-0.5B-Instruct\", \"reward\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T10:04:13.967871300Z",
     "start_time": "2025-05-07T10:04:13.758603300Z"
    }
   },
   "id": "b2b69b162cc29557",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 1, 1],\n",
      "         [3, 3, 3]],\n",
      "\n",
      "        [[0, 0, 0],\n",
      "         [2, 2, 2]]])\n",
      "tensor([[[ 4,  5,  6],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [19, 20, 21]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    " \n",
    "# 创建一个大小为 (2, 4, 3) 的输入张量\n",
    "input_tensor = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]],\n",
    "    [[13, 14, 15], [16, 17, 18], [19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    " \n",
    "# 创建一个大小为 (2, 2) 的索引张量\n",
    "indices = torch.tensor([[1, 3], [0, 2]])\n",
    " \n",
    "# 从每个批次的张量中收集指定位置的元素\n",
    "gathered_values = torch.gather(input_tensor, dim=1, index=indices.unsqueeze(-1).repeat(1, 1, input_tensor.shape[-1]))\n",
    "\n",
    "print(indices.unsqueeze(-1).repeat(1, 1, input_tensor.shape[-1]))\n",
    "print(gathered_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-07T14:32:06.872354100Z",
     "start_time": "2025-05-07T14:32:06.805268300Z"
    }
   },
   "id": "60d62bdcfe367bc4",
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "openrlhf",
   "language": "python",
   "display_name": "openrlhf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
